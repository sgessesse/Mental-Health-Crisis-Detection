{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e1ad46-840b-486a-8a32-fe9d0d21a739",
   "metadata": {},
   "source": [
    " # Mental Health Crisis Detection Model Training\n",
    "\n",
    "This notebook handles the training of the model. It uses the training and validation datasets to train the model and implements early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b75a8c-b943-4e0e-bfa3-ee39804d6963",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "- **Base Model**: BERT-base-uncased (12-layer transformer)\n",
    "- **Customizations**:\n",
    "  - 30% Hidden dropout\n",
    "  - 10% Attention dropout\n",
    "  - Frozen embeddings\n",
    "- **Training**:\n",
    "  - 3 Epochs (Early convergence observed)\r\n",
    "  - 2e-5 Learning rate (BERT standard)\r\n",
    "  - 32 Batch size + 4-step gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f40724-dd6f-4f95-8c99-6a4cfaf4a4d1",
   "metadata": {},
   "source": [
    "## Training Philosophy:\n",
    "- Recall-focused: Prioritize detecting all potential crisis cases\n",
    "- Overfit Mitigation: Validation set monitoring, regularization\n",
    "- Resource Efficiency: Mixed precision FP16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b3d8a5-f1af-4f05-b7b2-993dcdbb44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0104b5f-6819-44af-874f-b645ef3f63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a87bb86-b938-4e0d-b0ff-d65382db1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenize function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Convert text to BERT-compatible tokens\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'input_ids', 'attention_mask', etc.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True,  # Limit to 512 tokens\n",
    "        padding='max_length',  # Pad shorter sequences\n",
    "        max_length=512  # BERT's maximum capacity\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0499d3ec-857c-4434-84c4-8aac39c604f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute_metrics function to calculate recall\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Custom evaluation metric (Recall-focused)\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1) # Convert logits to class IDs\n",
    "    recall = recall_score(labels, predictions, average='binary')  # SUICIDAL class recall\n",
    "    return {'recall': recall}  # Primary success metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ab0b8a-437a-4d82-88b6-98bfb581eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train):\n",
    "    \"\"\"Complete training workflow\n",
    "    \n",
    "    Returns:\n",
    "        model: Fine-tuned BERT\n",
    "        tokenizer: Trained tokenizer\n",
    "        label_encoder: Class-label mapper\n",
    "    \"\"\"\n",
    "    # Encode text labels to integers\n",
    "    label_encoder = LabelEncoder() # SUICIDAL=1, NON-SUICIDAL=0\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train) # Numeric labels\n",
    "\n",
    "    # Create Hugging Face dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        'text': X_train.tolist(),  # Input posts\n",
    "        'labels': y_train_encoded.tolist()  # Encoded labels\n",
    "    })\n",
    "    \n",
    "    # Tokenization (CPU-bound)\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Model initialization\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', \n",
    "        num_labels=len(label_encoder.classes_), # Binary classification\n",
    "    )\n",
    "     # Architectural modifications\n",
    "    model.config.hidden_dropout_prob = 0.3 # Reduce overfittin\n",
    "    model.config.attention_probs_dropout_prob = 0.1 # Sparse attention\n",
    "\n",
    "   # Freeze embeddings (Transfer learning)\n",
    "    for param in model.bert.embeddings.parameters():\n",
    "        param.requires_grad = False # Keep pretrained embeddings static\n",
    "\n",
    "    # Training configuration\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results', # Logging directory\n",
    "        evaluation_strategy='epoch', # Validation every epoch\n",
    "        learning_rate=2e-5, # Standard for BERT fine-tuning\n",
    "        per_device_train_batch_size=32,  # Fits GPU memory\n",
    "        per_device_eval_batch_size=32,   \n",
    "        num_train_epochs=3, # Early stopping would extend this\n",
    "        weight_decay=0.01, # L2 regularization\n",
    "        logging_dir='./logs',\n",
    "        fp16=True,  # 16-bit training\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 32*4=128\n",
    "    )\n",
    "\n",
    "    # Split dataset into train and eval\n",
    "    train_dataset = tokenized_datasets.shuffle(seed=42).select(range(len(tokenized_datasets) - 500)) # Training data\n",
    "    eval_dataset = tokenized_datasets.shuffle(seed=42).select(range(len(tokenized_datasets) - 500, len(tokenized_datasets))) # 500-sample validation\n",
    "\n",
    "    # Trainer initialization\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer), # Dynamic padding\n",
    "        compute_metrics=compute_metrics,  # Recall tracking\n",
    "    )\n",
    "\n",
    "    # Model training\n",
    "    trainer.train()\n",
    "\n",
    "    return model, tokenizer, label_encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
