{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5935b278-43a6-4739-9f17-3762f5718940",
   "metadata": {},
   "source": [
    "# Mental Health Crisis Detection Data Preprocessing\n",
    "\n",
    "This notebook handles the loading and preprocessing of Suicide and Depression Detection dataset from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f9fc3-9847-466f-86ad-946b59b783af",
   "metadata": {},
   "source": [
    "## Data Cleaning Stages\n",
    "1. **Sanitization**:\n",
    "   - URL removal\n",
    "   - Special character stripping\n",
    "2. **Normalization**:\n",
    "   - Lowercasing\n",
    "   - Stopword removal (NLTK English)\n",
    "3. **Quality Control**:\n",
    "   - NaN removal\n",
    "   - Column standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753d935-2ea1-466d-9d42-4289611909c9",
   "metadata": {},
   "source": [
    "## Preprocessing Rationale\n",
    "- URL/Special Char Removal: Reduces noise in embeddings\n",
    "- Stopword Removal: Focuses on meaningful terms\n",
    "- Lowercasing: Matches BERT's uncased architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbe61ca-d739-445e-a41a-b6936614a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1b117e-bc54-4d11-8895-ee9dcb98628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sem_w\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f329269-5bcd-494c-852e-a5c6c346f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Sanitize raw dataframe\n",
    "    \n",
    "    Args:\n",
    "        df: Raw dataframe with 'text' and 'class' columns\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned dataframe ready for preprocessing\n",
    "    \"\"\"\n",
    "    df.drop(columns=df.columns[0], inplace=True)  # Drop the first column (index)\n",
    "    df.dropna(inplace=True)  # Handle missing values\n",
    "    df['text'] = df['text'].apply(remove_urls)  # Remove URLs\n",
    "    df['text'] = df['text'].apply(clean_special_chars)  # Remove non-alphanumeric characters\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b1c2181-7c1b-4e46-8125-8c3076f03f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs using regex pattern matching\"\"\"\n",
    "    return re.sub(r'http\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15186173-24f8-422c-bf05-bddd397c14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(text):\n",
    "    \"\"\"Retain only alphanumeric characters and whitespace\"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc434c12-6107-4893-b049-c593d1b24ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    \"\"\"Text normalization pipeline\"\"\"\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.lower().split() if word not in STOP_WORDS]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544c39ef-38ac-421d-93b9-cd5e869e76a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path):\n",
    "    \"\"\"End-to-end data preparation\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Stratified splits\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path) # Load raw data\n",
    "    df = clean_data(df) # Sanitization\n",
    "    df = preprocess_text(df) # Normalization\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['text'], df['class'], test_size=0.2, random_state=42) # Industry-standard split\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
