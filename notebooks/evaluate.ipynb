{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985cba49-fdf6-47b9-adf2-210532479232",
   "metadata": {},
   "source": [
    "# Mental Health Crisis Detection Evaluation\n",
    "This notebook handles the evaluation of the trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd09ebdb-88cd-4b4f-81ed-02747cc6b2f2",
   "metadata": {},
   "source": [
    "## Key Metrics\n",
    "- **F1 Score**: Balanced measure of precision and recall, crucial for imbalanced datasets\n",
    "- **Recall**: Particularly important to minimize false negatives in suicide detection\n",
    "- **Batch Processing**: Efficient GPU utilization with batch size 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e67fc4-8b38-4938-be3c-99a0700989c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52fe7b6-172d-4a99-a46b-334f59fd7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, X_test, y_test, label_encoder, batch_size=256):\n",
    "    \"\"\"Evaluate BERT model performance on test data\n",
    "    \n",
    "    Args:\n",
    "        model: Fine-tuned BERT model\n",
    "        tokenizer: BERT tokenizer\n",
    "        X_test: List of text samples\n",
    "        y_test: Ground truth labels\n",
    "        label_encoder: Fitted LabelEncoder for class mapping\n",
    "        batch_size: Batch size for GPU optimization\n",
    "        \n",
    "    Returns:\n",
    "        f1: Macro F1 score\n",
    "        recall: Recall score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Batch processing to handle large datasets and prevent GPU OOM\n",
    "    for i in range(0, len(X_test), batch_size):\n",
    "        batch_texts = X_test[i:i + batch_size].tolist()\n",
    "        \n",
    "        # Tokenization with truncation/padding for BERT input\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512 # Matches BERT's max sequence length\n",
    "        ).to(device)\n",
    "        \n",
    "        # Disable gradient calculation for inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Get class predictions from logits\n",
    "        batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        predictions.extend(batch_preds)\n",
    "        # Clear GPU cache after each batch\n",
    "        torch.cuda.empty_cache() # Clear GPU cache between batches\n",
    "    \n",
    "    # Convert string labels to encoded integers\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    f1 = f1_score(y_test_encoded, predictions)\n",
    "    recall = recall_score(y_test_encoded, predictions)\n",
    "    \n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    return f1, recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
